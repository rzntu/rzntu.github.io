[{"authors":["admin"],"categories":null,"content":"I am a Quantitative Researcher in Singapore. I completed my Ph.D studies at Nanyang Technological University supervised by Prof. Mao Kezhi. My Ph.D Dissertation title is Representation Learning for Sentences and Documents. Before that I obtained my undergraduate degree from Southeast University. In SEU, my FYP thesis named Stochastic Resonance for Machine Fault Diagnosis was guided by Prof. Yan Ruqiang. I have several years experience in developing machine learning projects (from POC to production) across various domains. My main research interests are machine learning, natural language processing and predictive modelling. Currently, I am exploring the application of machine learning on algorithmic trading.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rzntu.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Quantitative Researcher in Singapore. I completed my Ph.D studies at Nanyang Technological University supervised by Prof. Mao Kezhi. My Ph.D Dissertation title is Representation Learning for Sentences and Documents. Before that I obtained my undergraduate degree from Southeast University. In SEU, my FYP thesis named Stochastic Resonance for Machine Fault Diagnosis was guided by Prof. Yan Ruqiang. I have several years experience in developing machine learning projects (from POC to production) across various domains.","tags":null,"title":"RUI ZHAO","type":"authors"},{"authors":null,"categories":null,"content":"This module provides students a deep overview of various advanced machine learning techniques applied to business analytics tasks. The focus of this course will be the key and intuitive idea behind machine learning models and hands-on examples instead of theoretical analysis. The tentative topics include machine learning pipeline, unsupervised learning, structure learning, Bayesian learning, deep learning and generative models\n","date":1575331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1575504000,"objectID":"e152832b2ed17e0ba65afb79ab3ba876","permalink":"https://rzntu.github.io/courses/course3/","publishdate":"2019-12-03T00:00:00Z","relpermalink":"/courses/course3/","section":"courses","summary":"Instructor, NUS, 2020 Spring","tags":null,"title":"BT5153 Applied Machine Learning for Business Analytics","type":"docs"},{"authors":null,"categories":null,"content":" Course Links  MIT1806 Linear Algebra EE364a: Convex Optimization CS224d Deep Learning for Natural Language Processing Introduction to Gaussian Process Machine Learning CS 285 Deep Reinforcement Learning Machine Learning and Prediction in Economics and Finance Matrix Methods in Data Analysis, Signal Processing, and Machine Learning  ","date":1570060800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570060800,"objectID":"1d6a11b07112c00a5bf1d8e0fc15ac9b","permalink":"https://rzntu.github.io/links/doc2/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/links/doc2/","section":"links","summary":"Some awesome talks for Machine Learning and Deep Learning that I followed.","tags":null,"title":"Links for open courses","type":"docs"},{"authors":null,"categories":null,"content":" Data Visualization  How to Think about Data Visualization  Data Visualizations Codes  Model Interpretation  Kaggle Tutorial for Machine Learning Explainability Book: A Guide for Making Black Box Models Explainable  ","date":1570060800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570060800,"objectID":"ac2c8efa5c9dba42a3c57ed592743b9a","permalink":"https://rzntu.github.io/links/doc1/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/links/doc1/","section":"links","summary":"Data Visualization, Model Interpretation and etc.","tags":null,"title":"Useful links for Data Science","type":"docs"},{"authors":null,"categories":null,"content":"This module aims to prepare graduate students pursuing a master degree in business analytics for specific topics in business analytics which is of sufficient interest in the current context. The module will investigate the integration of business know-how and advanced analytics technologies, such as machine learning and AI. Students are advised to look into content detail for specific offering during the year.\nIn this offering, we impart the techniques of web mining to students who are interested in understanding ways of retrieving valuable information from the web and various social media. It aims to teach students various concepts, methods and tools in mining Web data in the form of unstructured Web hyperlinks, page contents, and usage logs to uncover deep business insights and knowledge for business implications that are embedded in the billions of Web pages and servers. The focus of this course will be on text mining, where we look at mainly unstructured text. Methods of delivery of lessons include lectures and case study discussions. This course is delivered using Python.\n","date":1570060800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570060800,"objectID":"41e9fa2800de646d7f3b36690cd26cfb","permalink":"https://rzntu.github.io/courses/course2/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/courses/course2/","section":"courses","summary":"Instructor, NUS, 2019 Spring","tags":null,"title":"BT5153 Topics in Business Analytics","type":"docs"},{"authors":null,"categories":null,"content":" This course is an intermediate level 5-week intensive training program. In these 5 week, you will be fully exposed to deep learning and data science both online, offline and with your supportive community, and build up your project portfolio to achieve your career dreams.\n 1.5 hours teaching + 1.5 hours hands-on in-class project per week In-class Kaggle Competition  The lesson table is given as:\nWeek 1  Basic concepts behind NLP and Deep Learning Representation learning and TF-IDF Models Stochastic gadients descent Softmax and cross entropy error  In-class Project Implement a basic Softmax layer from scratch (project)\nWeek 2  Non-linearities behind neural networks Feed-forward computation for NN Back-propagation for NN Overfitting, Activation functions, Regularization  In-class Project Tutorial on TensorFlow\nWeek 3  Count-based Word representation Neural network-based word2vec Inherent connections between Word2vec and TF-IDF models Recurrent Neural Networks and Language Model  In-class Project Word vectors training model\nWeek 4  Language Model Gradient Vanishing and Exploding Bi-directional Extension Cell Extension: LSTM or GRU  In-class Project LSTM sentiment analysis\nWeek 5  A brief review on CNN Implement multi-channel CNNs for sentiment analysis Design the code structure following a standard tensorflow template which can provide a good practice for our daily job.  ","date":1570060800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570060800,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://rzntu.github.io/courses/example/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Instructor, YiDu AI, 2018","tags":null,"title":"Data Science Meets Deep Learning","type":"docs"},{"authors":null,"categories":null,"content":" Resources  Regular Expression  ","date":1570060800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1570060800,"objectID":"3d0adb2a1d2f8ada9b80f4e0bc304142","permalink":"https://rzntu.github.io/links/doc3/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/links/doc3/","section":"links","summary":"Some useful links","tags":null,"title":"","type":"docs"},{"authors":[],"categories":[],"content":" Selected Papers from AQR Capital Management Even AQR have not so decent performances during recent years, it is still meaningful to read some academic papers related to quant trading/markets.\nThe Great Divide For efficient market hypothesis that the simple statement that security prices fully reflect all available information, there is always a debate. Here, AQR founders provide their own insights toward this debate.\nFirstly, they pointed out that to make any statement about market efficiency, you need to make some assertion of how the market should reflect information. Therefore, here is a joint hypothesis. For example, CAPM model defines that how prices are set. Therefore, we can not tell which of two ideas you are rejecting from the evidences.\nFor the joint of CAPM and EMH, the microchallenges are value and momentum methods. Value can be the HML (High minus low) that goes long a diversified portfolio of high booktoprice ratios and short the one of low booktoprice ratios. Momentum can be trend-following. To reject the CAPM and EMH, two kinds of arguments have been proposed as:\n risk: market beta is not the only source of risk. The higher expected return of cheap stocks is rational, as it reflects higher risk.\n behaviorists: behavior biases cause prices to get too high or too low. For instance, investors may overextrapolate both good and bad news thus long too much or short too much.\n  TBU.\nBuffett\u0026rsquo;s Alpha In this paper, they did a very detailed empirical analysis of Buffett\u0026rsquo;s results to hack the investment style. They are trying to replicate it in an systemmatic way.\n This paper suggest that Buffett’s success is neither luck nor magic but is a reward for a successful implementation of value and quality exposures that have historically produced high returns.\n Buffett applies a lverage of about 1.7 to 1.\n Stock-selection is the key: buy stocks that are safe (with low beta and low volatility), cheap (i.e., value stocks with low price-to-book ratios) and of high quality (profitable, stable, and growing stocks with high payout ratios)\n Although optimistic asset managers often claim to be able to achieve Sharpe ratios above 1 or 2 and many chief investment officers seek similarly high performance numbers, our results suggest that long-term investors might do well to set a realistic performance goal and brace themselves for the tough periods that even Buffett has experienced. Indeed, because Buffett became one of the richest people in the world with a Sharpe ratio of 0.79, most investors should seek to actually deliver a Sharpe ratio somewhere between this number and the market’s Sharpe ratio, which was around 0.5 during this sample period, rather than making suboptimal investments in a futile attempt to consistently reach a much higher number\n  ","date":1578491662,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578840862,"objectID":"80c070a68a459e3f9c89a577d4e8fef8","permalink":"https://rzntu.github.io/post/20-for-twenty/","publishdate":"2020-01-08T21:54:22+08:00","relpermalink":"/post/20-for-twenty/","section":"post","summary":"random notes","tags":[],"title":"20 For Twenty","type":"post"},{"authors":["Jinjiang Wang","Rui Zhao","Robert X Gao"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9d436ce82d93d379e568d26f37429661","permalink":"https://rzntu.github.io/publication/wang-2020-tf/","publishdate":"2020-01-03T09:00:43.172544Z","relpermalink":"/publication/wang-2020-tf/","section":"publication","summary":"The variability of machinery fault signatures causes the data  samples  to  follow  different  distributions  under  various operating  conditions,  which  poses  significant  challenges  on autonomous diagnosis based on machine learning techniques. This paper presents a new  transfer learning method for cross-domain feature learning  by  mitigating  the  domain  difference  caused  by various operating conditions for machinery autonomous diagnosis. More specifically, a factor analysis based transfer learning method is formulated and named as probabilistic transfer factor analysis. It  seeks  a  new  feature  space  across  different  domains corresponding to various operating conditions, and then transfers the  original  features  into  a  low-dimensional  latent  space  via feature  extraction to  minimize  domain  difference and  preservedata  properties.  The  learned  features  by  probabilistic  transfer factor analysis minimize the domain difference, and are then used to construct the machinery autonomous diagnosis model based on machine learning techniques (e.g.  support  vector  machine).  The effectiveness of the probabilistic transfer factor analysis method is demonstrated in  the  experimental  tests  for  a  gearbox  diagnosis cross  various  operating  conditions  comparing  with  traditional feature extraction and transfer learning techniques.","tags":null,"title":"Probabilistic Transfer Factor Analysis for Machinery Autonomous Diagnosis Cross Various Operating Conditions","type":"publication"},{"authors":[],"categories":[],"content":" Advances in Financial Machine Learning Lots of insights on practical aspects of machine learning in finances have been shared in this book. Some of them are quite useful. But the author sometimes tend to make the simple or very intuitive problem/solution seems very complex and fuzzy. In this random notes, I may write down some of my random ideas.\nCH2: Financial Data Structures The main topic in this chapter is that how do we form the bar. Usually, the charts/technical analysis are performed upon the time-sampled bar. Each bar means the information sampled from a fixed interval. Then, the drawback is that time bars oversample information during low-activity periods and undersample information during high-activity periods. The low-activity and high-activity may refer to the trading volume. Then, several information-driven bars have been proposed. The core idea is that we want to sample the information evenly. Different information measures lead to different methods. For example, the volume, the volume*price, the number of ticks. And it is claimed that sampling as a function of trading activity allows us to achieve returns closer to IID Normal. (I am not sure why is it)\nAt last, I think this bar sampling technique is one of representation learning. In machine learning, how do we convert the unstructured data into structured one. Here, it do the same thing. Time series to the bar that be stored in a csv format.\nFrom my own experiments, MACD applied on some information-driven bars have not shown good performance. I am not sure it is the problems behind the technical analysis or the bars themselves.\nCH3: Labeling How to define y in machine learning for trading strategy. The authors propose a triple-barrier methods, which involve some interesting ideas:\nfor example, we want to label one tradable product at time T.\n Define a maximum holding period N, then we will look at T: T+N\n Based on the recent volatility, two horizontal lines are introduced indicating profit-taking and cut-loss lines.\n The label will depend on the order of the touched lines.\n  The advantage is that it consider the price change path instead of the absoulte change p(T+N) - p(T). However, the concern may that how do we define the horizontal lines. It involves some hyperparameters, which may be very tricky. In addition, it is also unsure that in which frequency, we should look at the touched lines. For example, if we label the data at 30 mins, we can use any time frequency higher than 30 mins to check which line will be touched firstly.\nAt the same time, the meta-label was also introduced by the author. At the same time, in ml research community, meta-learnin is also popular. The meta-label idea is that we add second layer label/classifier. The first layer will be any trading strategy, which will do timing/generate buy sell points. Then, the second layer label will be the size betting. The second layer label will be only 1 and 0, which means if the trade we entered at time T, can we have profit. The classifier may have the prob. output. This continus and normalized output can be the size we predict.\nCh11: The Dangers of Backtesting This chapter is talking about the issues of data leakage in the context of trading algorithms development. Some sentences may be highlighed in case I made the same mistakes in my daily job.\n The point of a backtest is a sanity check on a number of variables, including bet sizing, turnover, resilience to costs, and behavior under a given scenario.\n The maddening thing about backtesting is that, the better you become at it, the more likely false discoveries will pop up.\n The purpose of backtesting is to discard bad models, not to improve them.\n What makes backtest overfitting so hard to access is that the probability of false positive changes with every new test conducted on the same dataset.\n  One of measures named Probability of Backtest Overfitting was proposed. The input is a return matrix: one dimension is time axis and the other dimension is the algorithms. The idea is that based on the training period, the best algo should perform above the median in the testing period. And simialr to cross-validation, PBO fakely generated lots of pairs of training and testing. Then, based on a large number of simulations, the probability score could be inferred.\n","date":1573221262,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577631262,"objectID":"577e6751fe4bc634fd7b47fbbae7cd5c","permalink":"https://rzntu.github.io/post/advances-in-financial-machine-learning/","publishdate":"2019-11-08T21:54:22+08:00","relpermalink":"/post/advances-in-financial-machine-learning/","section":"post","summary":"random notes","tags":[],"title":"Advances in Financial Machine Learning","type":"post"},{"authors":[],"categories":[],"content":" Diversified Managed Futures Trading CH2: Futures Data and Tools  Term Structure:\n The fair price of any position is the cost of hedging it, so if you can hedge something you can also price it. For financial futures such as index and bond ones, the interest rate is the main driver of the term structure shape that is less of a steep curve for financial curses than for deliverable ones. Backwardation can be caused by seasonality, interest rate conditions or unusual storage cost situations and is not uncommon for softs and perishable commodities.  Terms to describe the price change:\n Points: the smallest price increment change that on the left side of the decimal point\n Ticks: A point is composed of ticks. Tick size deterimes that how many ticks it taks to increase the point. E.g. four ticks to a point is the tick size of 0.25 Point Value * tick size = tick value five contracts at 100 and sell them at 110 with a point value of 10, then the profit is 10*5*10*1=500  Futures Sectors:\n Agricultural: relatively low internal correlation but small market Non-agricultural Commodities: Oil and natural Gas, Gold, so on. For natural gas, it is persistently sharp contango (highly complicated and nigh impossible storage and the seasonal demand patterns) Currencies: Be aware of the danger of stacking up USD risks. For example, long the CHF, long the euro, yen and son. They are all shorting USD.\n Equities: * Short side of equities usually has a very different profile from the long side. * When equiteis are in bull market, they can move slowly upwards for long periods of time. While on the downside, equity moves tend to be swifter and more violent. * Short side of CTA on equities struggle a lot. Think sharp drop-downs followed by v-shaped reversals.\n Rates:\n   CH4: Two Basic Trend-following Strategies  Correlations between strategies:\nCompute the log return ln(pi/pi-1)\n The secret sauce is not in the buy and sell rules:\nThe author only presented two very simple strategies:\n Long if the 50-day MA is above the 100-day MA, short if the 50-day MA is less than the 100-day MA. Long if the closing price is the highest close price in the past 50 days and short if the closing price is the lowest close price. Position: (0.2% * Equity) / (ATR * pointValue) = number of contract With stop loss criteria: if long position, it will be closed if it has moved three ATR units down from its highest price after this trade. If short position. It will be closed if it has moved three ATR units lower from its lowest closing price since the position was opened. Add a trend filter which makes sure that we only trade in the direction of the main trend and that we do not get whipsawed by going in and out of positions every couple of days.   CH5-6: Depth Analysis of Trend-Following Performance  Long side showing a smoother upwards profile and the short side a more dynamic burst-style return. The short side is far less profitable and in fact often ends up losing money for extended periods of time; but it still adds value by acting a volatility stabiliser as well as providing substantial gain in extreme market situations.\n Diversified trend following is prone to extreme periods of success followed by sharp decliunes and sideways choppy markets.\n Generally speaking, commodities tend to do better when dollar is trending down.\n For trending algos, the short side is a much needed stabiliser over time\n  ","date":1560002062,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562597662,"objectID":"9286d2d4ad6e4696d658cc1c85293c6f","permalink":"https://rzntu.github.io/post/following-the-trend/","publishdate":"2019-06-08T21:54:22+08:00","relpermalink":"/post/following-the-trend/","section":"post","summary":"random notes","tags":[],"title":"Following the Trend","type":"post"},{"authors":["Rui Zhao","Ruqiang Yan","Zhenghua Chen","Kezhi Mao","Peng Wang","Robert X Gao"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"33207574aa64e81b827d24fc2b9f6ef2","permalink":"https://rzntu.github.io/publication/zhao-2019-deep/","publishdate":"2019-10-03T09:00:43.168547Z","relpermalink":"/publication/zhao-2019-deep/","section":"publication","summary":"Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In addition, an experimental study on the performances of these approaches has been conducted, in which the data and code have been online. Finally, some new trends of DL-based machine health monitoring methods are discussed.","tags":null,"title":"Deep learning and its applications to machine health monitoring","type":"publication"},{"authors":["Jinjiang Wang","Peilun Fu","Laibin Zhang","Robert X Gao","Rui Zhao"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c7ac660a0184b795667c230d9d266841","permalink":"https://rzntu.github.io/publication/wang-2019-multi/","publishdate":"2019-10-03T09:00:43.172544Z","relpermalink":"/publication/wang-2019-multi/","section":"publication","summary":"Condition monitoring and fault diagnosis are of significance to improve the safety and reliability of motors, given their widespread applications in virtually every branch of the industry. Sequential data modeling based on recurrent neural network (RNN) and its variants have drawn increasing attention because the temporal nature of motor signals can be well leveraged for motor analysis. One common drawback of prior research is that signals measured on motors are typically analyzed with a fixed time window, making it difficult to trade off between global state estimation and local feature extraction. This paper presents a deep learning-based model termed Multi-Resolution \u0026 multi-Sensor Fusion Network (MRSFN) for motor fault diagnosis, through multi-scale analysis of motor vibration and stator current signals. Specifically, vibration and current signals are first segmented by analysis windows of varying lengths to create a new data stream for the joint representation and temporal encoding of the original sensor signals, based on two network structures: convolutional neural network (CNN) and long short-term memory (LSTM). The advantage of the developed method is that it automatically learns the discriminative features through the network training process, without requiring manual feature selection as is typically the case in prior methods. By considering the temporal dependence of the signals being analyzed, the developed multi-resolution fusion technique not only improves the effectiveness of feature extraction but is also adaptive to varying motor speed. Two case studies demonstrate the advantages of the developed method.","tags":null,"title":"Multi-level information fusion for induction motor fault diagnosis","type":"publication"},{"authors":["Zhenghua Chen","Rui Zhao","Qingchang Zhu","Mustafa K Masood","Yeng Chai Soh","Kezhi Mao"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b9fcccbf0ecf5f567aa4b50e1338aa75","permalink":"https://rzntu.github.io/publication/chen-2017-building/","publishdate":"2019-10-03T09:00:43.159552Z","relpermalink":"/publication/chen-2017-building/","section":"publication","summary":"Buildings consume quite a lot of energy; hence, the issue of building energy efficiency has attracted a great deal of attention in recent years. A key factor in achieving this objective is occupancy information that directly impacts on energy-related building control systems. In this paper, we leverage on environmental sensors that are nonintrusive and cost-effective for building occupancy estimation. Our result relies on feature engineering and learning. The conventional feature engineering requires one to manually extract relevant features without a clear guideline. This blind feature extraction is labor intensive and may miss some significant implicit features. To address this issue, we propose a convolutional deep bidirectional long short-term memory (CDBLSTM) approach that contains a convolutional network and a deep structure to automatically learn significant features from the sensory data without human intervention. Moreover, the long short-term memory networks are able to capture temporal dependencies in the data and the bidirectional structure can take the past and future contexts into consideration for the final identification of occupancy. We have conducted real experiments to evaluate the performance of our proposed CDBLSTM approach. Instead of estimating the exact number of occupants, we attempt to identify the range of occupants, i.e., zero, low, medium, and high, which is adequate for most of building control systems. The experimental results indicate the effectiveness of our proposed approach compared with the state-of-the-art methods.","tags":null,"title":"Building occupancy estimation with environmental sensors via CDBLSTM","type":"publication"},{"authors":["Wenjun Sun","Rui Zhao","Ruqiang Yan","Siyu Shao","Xuefeng Chen"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"92e275be3c0bd2a7a4bfdedfb2813b55","permalink":"https://rzntu.github.io/publication/sun-2017-convolutional/","publishdate":"2019-10-03T09:00:43.147558Z","relpermalink":"/publication/sun-2017-convolutional/","section":"publication","summary":"A convolutional discriminative feature learning method is presented for induction motor fault diagnosis. The approach firstly utilizes back-propagation (BP)-based neural network to learn local filters capturing discriminative information. Then, a feed-forward convolutional pooling architecture is built to extract final features through these local filters. Due to the discriminative learning of BP-based neural network, the learned local filters can discover potential discriminative patterns. Also, the convolutional pooling architecture is able to derive invariant and robust features. Therefore, the proposed method can learn robust and discriminative representation from the raw sensory data of induction motors in an efficient and automatic way. Finally, the learned representations are fed into support vector machine classifier to identify six different fault conditions. Experiments performed on a machine fault simulator indicate that compared with the current state-of-the-art methods, the proposed method shows significant performance gains, and it is effective and efficient for induction motor fault diagnosis.","tags":null,"title":"Convolutional discriminative feature learning for induction motor fault diagnosis","type":"publication"},{"authors":["Rui Zhao","Kezhi Mao"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0a5b4f9d7e84a0e1c741436a63726382","permalink":"https://rzntu.github.io/publication/zhao-2017-fuzzy/","publishdate":"2019-10-03T09:00:43.152556Z","relpermalink":"/publication/zhao-2017-fuzzy/","section":"publication","summary":"One key issue in text mining and natural language processing (NLP) is how to effectively represent documents using numerical vectors. One classical model is the Bag-of-Words (BoW). In a BoW-based vector representation of a document, each element denotes the normalized number of occurrence of a basis term in the document. To count the number of occurrence of a basis term, BoW conducts exact word matching, which can be regarded as a hard mapping from words to the basis term. BoW representation suffers from its intrinsic extreme sparsity, high dimensionality, and inability to capture high-level semantic meanings behind text data. To address the above issues, we propose a new document representation method named Fuzzy Bag-of-Words (FBoW) in this paper. FBoW adopts a fuzzy mapping based on semantic correlation among words quantified by cosine similarity measures between word embeddings. Since word semantic matching instead of exact word string matching is used, the FBoW could encode more semantics into the numerical representation. In addition, we propose to use word clusters instead of individual words as basis terms and develop Fuzzy Bag-of-WordClusters (FBoWC) models. Three variants under the framework of FBoWC are proposed based on three different similarity measures between word clusters and words, which are named as FBoWCmean, FBoWCmax and FBoWCmin, respectively. Document representations learned by the proposed FBoW and FBoWC are dense and able to encode high-level semantics. The task of document categorization is used to evaluate the performance of learned representation by the proposed FBoW and FBoWC methods. The results on seven real word document classification datasets in comparison with six document representation learning methods have shown that our methods FBoW and FBoWC achieve the highest classification accuracies.","tags":null,"title":"Fuzzy bag-of-words model for document representation","type":"publication"},{"authors":["Rui Zhao","Dongzhe Wang","Ruqiang Yan","Kezhi Mao","Fei Shen","Jinjiang Wang"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3c2c76b2d55bf4ee728ce4641a1366e2","permalink":"https://rzntu.github.io/publication/zhao-2017-machine/","publishdate":"2019-10-03T09:00:43.164552Z","relpermalink":"/publication/zhao-2017-machine/","section":"publication","summary":"In modern industries, machine health monitoring systems (MHMS) have been applied wildly with the goal of realizing predictive maintenance including failures tracking, downtime reduction and assets preservation. In the era of big machinery data, data-driven MHMS have achieved remarkable results in the detection of faults after the occurrence of certain failures (diagnosis) and prediction of the future working conditions and the remaining useful life (prognosis). The numerical representation for raw sensory data is the key stone for various successful MHMS. Conventional methods are labor-extensive as they usually depend on handcrafted features, which require expert knowledge. Inspired by the success of deep learning methods that redefine representation learning from raw data, we propose local feature-based gated recurrent unit networks (LFGRU). It is a hybrid approach that combines handcrafted feature design with automatic feature learning for machine health monitoring. Firstly, features from windows of input time series are extracted. Then, an enhanced bi-directional GRU network is designed and applied on the generated sequence of local features to learn the representation. A supervised learning layer is finally trained to predict machine condition. Experiments on three machine health monitoring tasks: tool wear prediction, gearbox fault diagnosis and incipient bearing fault detection verify the effectiveness and generalization of the proposed LFGRU.","tags":null,"title":"Machine health monitoring using local feature-based gated recurrent unit networks","type":"publication"},{"authors":["Jinjiang Wang","Junyao Xie","Rui Zhao","Kezhi Mao","Laibin Zhang"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"285ea265ebc05d69be19e34a9511bd9a","permalink":"https://rzntu.github.io/publication/wang-2016-new/","publishdate":"2019-10-03T09:00:43.135564Z","relpermalink":"/publication/wang-2016-new/","section":"publication","summary":"The features extracted from multisensory measurements can be used to characterize machinery conditions. However, the nonlinearity and uncertainty presented in machinery degradation process pose challenges on feature selection and fusion in machinery condition monitoring. To alleviate these issues, this paper presents a new probabilistic nonlinear feature selection and fusion method, named probabilistic kernel factor analysis (PKFA). First, the mathematical structure of the PKFA is formulated incorporating kernel techniques on the basis of conventional factor analysis (FA). Next, a PKFA-based machining tool condition monitoring model with support vector regression is presented. The effectiveness of the scheme is experimentally verified on a machining tool testbed. The experimental results show that the proposed PKFA method provides more accurate tool condition prediction than using all initially extracted features and other feature selection techniques (e.g., kernel principal component analysis and conventional FA), and thus confirms its utility as an effective tool for machining tool condition assessment.","tags":null,"title":"A new probabilistic kernel factor analysis for multisensory data fusion: application to tool condition monitoring","type":"publication"},{"authors":["Rui Zhao","Kezhi Mao"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"bc28c0ac82ea84ebd246580479fe0b2a","permalink":"https://rzntu.github.io/publication/zhao-2016-cyberbullying/","publishdate":"2019-10-03T09:00:43.131567Z","relpermalink":"/publication/zhao-2016-cyberbullying/","section":"publication","summary":"As a side effect of increasingly popular social media, cyberbullying has emerged as a serious problem afflicting children, adolescents and young adults. Machine learning techniques make automatic detection of bullying messages in social media possible, and this could help to construct a healthy and safe social media environment. In this meaningful research area, one critical issue is robust and discriminative numerical representation learning of text messages. In this paper, we propose a new representation learning method to tackle this problem. Our method named Semantic-Enhanced Marginalized Denoising Auto-Encoder (smSDA) is developed via semantic extension of the popular deep learning model stacked denoising autoencoder. The semantic extension consists of semantic dropout noise and sparsity constraints, where the semantic dropout noise is designed based on domain knowledge and the word embedding technique. Our proposed method is able to exploit the hidden feature structure of bullying information and learn a robust and discriminative representation of text. Comprehensive experiments on two public cyberbullying corpora (Twitter and MySpace) are conducted, and the results show that our proposed approaches outperform other baseline text representation learning.","tags":null,"title":"Cyberbullying detection based on semantic-enhanced marginalized denoising auto-encoder","type":"publication"},{"authors":["Yong Zhang","Meng Joo Er","Rui Zhao","Mahardhika Pratama"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0ed1fa1d778935a479d135b50e40a1cb","permalink":"https://rzntu.github.io/publication/zhang-2016-multiview/","publishdate":"2019-10-03T09:00:43.140563Z","relpermalink":"/publication/zhang-2016-multiview/","section":"publication","summary":"Multidocument summarization has gained popularity in many real world applications because vital information can be extracted within a short time. Extractive summarization aims to generate a summary of a document or a set of documents by ranking sentences and the ranking results rely heavily on the quality of sentence features. However, almost all previous algorithms require hand-crafted features for sentence representation. In this paper, we leverage on word embedding to represent sentences so as to avoid the intensive labor in feature engineering. An enhanced convolutional neural networks (CNNs) termed multiview CNNs is successfully developed to obtain the features of sentences and rank sentences jointly. Multiview learning is incorporated into the model to greatly enhance the learning capability of original CNN. We evaluate the generic summarization performance of our proposed method on five Document Understanding Conference datasets. The proposed system outperforms the state-of-the-art approaches and the improvement is statistically significant shown by paired t-test","tags":null,"title":"Multiview convolutional neural networks for multidocument extractive summarization","type":"publication"},{"authors":["Rui Zhao","Kezhi Mao"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"079b75b3e303f8d10115c7bd7100ee04","permalink":"https://rzntu.github.io/publication/zhao-2016-topic/","publishdate":"2019-10-03T09:00:43.145558Z","relpermalink":"/publication/zhao-2016-topic/","section":"publication","summary":"In recent years, deep compositional models have emerged as a popular technique for representation learning of sentence in computational linguistic and natural language processing. These models normally train various forms of neural networks on top of pre-trained word embeddings using a taskspecific corpus. However, most of these works neglect the multisense nature of words in the pre-trained word embeddings. In this paper we introduce topic models to enrich the word embeddings for multi-senses of words. The integration of the topic model with various semantic compositional processes leads to Topic-Aware Convolutional Neural Network (TopCNN) and Topic-Aware Long Short Term Memory Networks (TopLSTMs). Different from previous multi-sense word embeddings models that assign multiple independent and sense-specific embeddings to each word, our proposed models are lightweight and flexible frameworks that regard word sense as the composition of two parts: a general sense derived from a large corpus and a topicspecific sense derived from a task-specific corpus. In addition, our proposed models focus on semantic composition instead of word understanding. With the help of topic models, we can integrate the topic-specific sense at word-level before composition and sentence-level after composition. Comprehensive experiments on five public sentence classification datasets are conducted, and the results show that our proposed Topic-Aware deep compositional models produce competitive or better performance than other text representation learning methods.","tags":null,"title":"Topic-aware deep compositional models for sentence classification","type":"publication"}]